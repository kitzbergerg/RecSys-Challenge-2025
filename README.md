# Universal Behavioral Modeling Data Challenge

## Introduction

The goal was to create Universal Behavioral Profile, which is based on provided data (purchases, add to cart, remove
from cart, page visits and search query). This data is extracted and used to create a set of features for each client
this is a user representation which is then used for training a model for multiple prediction tasks.

## Leaderboard embeddings

The embeddings uploaded in the leaderboard can be found in the submission folder (gitlab). They were obtained using the
autoencoder model (the same which can be run under run.sh)
For the other models, the last available embeddings are provided in the respective folders within embeddings_provided (
jupyerhub)

## Dataset

To run experiments download the dataset (see [here](https://www.recsyschallenge.com/2025/), or use
the [direct link](https://data.recsys.synerise.com/dataset/ubc_data/ubc_data.tar.gz)).  
Extract the data to `data/original/` such that the directory `data/original/input` (and others) exists.

## Execution

To use our implementation, the following steps are needed. The best results so far were obtained using the autoencoder.

### Create embeddings using (baseline extended) calculators

To create the embeddings on the full set, start run_calculators.sh in shell. The embeddings will be stored in the
embeddings folder.

```bash
./script/calculators.sh
```

### Create embeddings using the autoencoder

If you open the src/autoencoder_pipeline.py file, you'll find a Config class at the top where you can configure
parameters. Included is a feature to save and load features generated by the calculators, thereby saving time on
embedding generation.

```bash
./scripts/autoencoder.sh
```

### Create embeddings using the contrastive learning

```bash
./scripts/contrastive.sh
```

### Create embeddings using transformer

Training the transformer takes a long time (>15h on Radeon RX 7900 XTX). It also might require some manual tuning of
parameters like masking probabilities in training dataset.

`transformer.sh` gives a general idea of what has to be done. Note that just running this file won't work, as the
embeddings generation step requires setting the path to pytorch_lighting checkpoints.

```bash
./scripts/transformer.sh
```
